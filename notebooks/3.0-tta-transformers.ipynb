{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.models.eval_model' from '/home/mas-server/etu/nn/paraphrase_detection/notebooks/../src/models/eval_model.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use huggingface datasets\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import mlflow\n",
    "import importlib\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src import models\n",
    "\n",
    "importlib.reload(models)\n",
    "importlib.reload(models.transformer_models)\n",
    "importlib.reload(models.train_model)\n",
    "importlib.reload(models.eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/mas-server/etu/nn/paraphrase_detection/notebooks/mlruns/2', creation_time=1711215132962, experiment_id='2', last_update_time=1711215132962, lifecycle_stage='active', name='transformers', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mas-server/anaconda3/envs/ocean/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\", use_fast=True)\n",
    "def tokenize_function(samples):\n",
    "    return tokenizer(            \n",
    "        samples[\"question1\"], # first string\n",
    "        samples[\"question1\"], # second string\n",
    "        return_tensors=\"pt\", # return torch tensor\n",
    "        padding=\"max_length\", # Pad seqeunces\n",
    "        max_length=128, # Max len for padded seq\n",
    "        truncation=True, # Truncate string\n",
    "        return_token_type_ids=True, # Return mask for q1 and q2\n",
    "    )\n",
    "def collate_fn(data):\n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in data])\n",
    "    labels = torch.stack([example[\"labels\"] for example in data]).reshape(-1, 1)\n",
    "    masks = torch.stack([example[\"attention_mask\"] for example in data]).bool()\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\" : masks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67002de25d0c4b548559dde2b37f7be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/283000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ca1c8789f241c49bf25cd64cdc3a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/283000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17827ca40d1c47058531acae61ef5de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60643 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391b2520076e436da8c53cf6b8b5980b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60643 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b23216572b6460694b916b6f9593b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962d026bcdd44b4b9db28373935f4f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = '../data/processed'\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "VAL = 'val'\n",
    "text_datasets = {\n",
    "    x : Dataset.from_pandas(pd.read_csv(data_dir + \"/\" + x + \".csv\"))\n",
    "        # Tokenize questions\n",
    "        .map(tokenize_function, batched = True)\n",
    "        # Rename is_duplicate to labels\n",
    "        .map(lambda examples: {\"labels\": examples[\"is_duplicate\"]}, batched=True\n",
    "        )\n",
    "    for x in [TRAIN, VAL, TEST]\n",
    "}\n",
    "for dataset in text_datasets:\n",
    "    text_datasets[dataset].set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        text_datasets[x], batch_size = 128,\n",
    "        shuffle=True, collate_fn = collate_fn\n",
    "    )\n",
    "    for x in [TRAIN, VAL, TEST]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlflow_experiment(\n",
    "    exp_name,\n",
    "    model_config_path,\n",
    "    dataloaders,\n",
    "    epochs\n",
    "):\n",
    "    with open(model_config_path, \"r\") as f:\n",
    "        model_config = json.load(f)\n",
    "    model_type = model_config.get(\"type\", \"No type in config\")\n",
    "    if model_type == \"lstm\":\n",
    "        model = models.lstm_models.build_SimpleBiLSTM(model_config)\n",
    "    elif model_type == \"residual_lstm\":\n",
    "        raise NotImplemented()\n",
    "    elif model_type == \"transformer\":\n",
    "        model = models.transformer_models.build_transfomer(model_config)\n",
    "    with mlflow.start_run(run_name=exp_name):\n",
    "        print(f\"Model config: {model_config}\")\n",
    "        model.to(\"cuda\")\n",
    "        mlflow.log_params(model_config)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.set_tag(\"model_name\", model_type)\n",
    "        model = models.train_model.train_model(\n",
    "            model,\n",
    "            torch.nn.BCEWithLogitsLoss(),\n",
    "            optim.Adam(model.parameters(), weight_decay=1e-4),\n",
    "            dataloaders,\n",
    "            model_type,\n",
    "            epochs\n",
    "        )\n",
    "        models.eval_model.eval_model(model, dataloaders, model_type)\n",
    "        mlflow.pytorch.log_model(model, \"torch_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformer0.json']\n"
     ]
    }
   ],
   "source": [
    "config_paths = \"../models/\"\n",
    "configs = [x for x in os.listdir(config_paths) if x.endswith(\".json\") and x.startswith(\"transformer\")]\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config: {'vocab_size': 128000, 'embedding_dim': 80, 'maxlen': 128, 'nhead': 4, 'd_hid': 150, 'dropout': 0.5}\n",
      "Epoch 0/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 1/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 2/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 3/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 4/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 5/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 6/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 7/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 8/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 9/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 10/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 11/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 12/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 13/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 14/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 15/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 16/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 17/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 18/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 19/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 20/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 21/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 22/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 23/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 24/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 25/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 26/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 27/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 28/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 29/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 30/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 31/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 32/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 33/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 34/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 35/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 36/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 37/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 38/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 39/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 40/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 41/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 42/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 43/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 44/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 45/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 46/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 47/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 48/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "Epoch 49/50\n",
      "----------\n",
      "Training batch 2210/2211\n",
      "Validation batch 473/474\n",
      "\n",
      "Training completed in 23m 45s\n",
      "\n",
      "\n",
      "\\Testing completed in 0m 3s\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "# Disable tokenizers parallel to disable nasty warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "for conf in configs:\n",
    "    run_mlflow_experiment(\n",
    "        conf[:-5],\n",
    "        config_paths + conf,\n",
    "        dataloaders,\n",
    "        50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maslab_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

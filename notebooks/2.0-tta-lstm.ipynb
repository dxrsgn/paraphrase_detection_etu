{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.models.eval_model' from '/home/mas-server/etu/nn/paraphrase_detection/notebooks/../src/models/eval_model.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use huggingface datasets\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import mlflow\n",
    "import importlib\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src import models\n",
    "\n",
    "importlib.reload(models)\n",
    "importlib.reload(models.lstm_models)\n",
    "importlib.reload(models.train_model)\n",
    "importlib.reload(models.eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/mas-server/etu/nn/paraphrase_detection/notebooks/mlruns/1', creation_time=1711035737638, experiment_id='1', last_update_time=1711035737638, lifecycle_stage='active', name='lstms', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"lstms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mas-server/anaconda3/envs/ocean/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\", use_fast=True)\n",
    "def tokenize_function(samples):\n",
    "    return tokenizer(            \n",
    "        samples[\"question1\"], # first string\n",
    "        samples[\"question1\"], # second string\n",
    "        return_tensors=\"pt\", # return torch tensor\n",
    "        padding=\"max_length\", # Pad seqeunces\n",
    "        max_length=128, # Max len for padded seq\n",
    "        truncation=True, # Truncate string\n",
    "        return_token_type_ids=True, # Return mask for q1 and q2\n",
    "    )\n",
    "def collate_fn(data):\n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in data])\n",
    "    labels = torch.stack([example[\"input_ids\"] for example in data])\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6434a939974f138f1f07c978241d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/283000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b91b6439c04694abecf3e8d85ad8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/283000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5304c3545f924dd0b82a5f2d16ef777d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60643 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b6b6e8714b46ea9af31816cf7fb7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60643 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28006f106c847b791430cb4a7fae69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106d341566954eb6a11b131df39095a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = '../data/processed'\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "VAL = 'val'\n",
    "text_datasets = {\n",
    "    x : Dataset.from_pandas(pd.read_csv(data_dir + \"/\" + x + \".csv\"))\n",
    "        # Tokenize questions\n",
    "        .map(tokenize_function, batched = True)\n",
    "        # Rename is_duplicate to labels\n",
    "        .map(lambda examples: {\"labels\": examples[\"is_duplicate\"]}, batched=True\n",
    "        )\n",
    "    for x in [TRAIN, VAL, TEST]\n",
    "}\n",
    "for dataset in text_datasets:\n",
    "    text_datasets[dataset].set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        text_datasets[x], batch_size = 32,\n",
    "        shuffle=True, num_workers = 0,\n",
    "        collate_fn = collate_fn\n",
    "    )\n",
    "    for x in [TRAIN, VAL, TEST]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1, 2467, 1280,  ...,    0,    0,    0],\n",
       "         [   1,  369,  274,  ...,    0,    0,    0],\n",
       "         [   1,  458,  490,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   1,  458,  269,  ...,    0,    0,    0],\n",
       "         [   1,  458,  338,  ...,    0,    0,    0],\n",
       "         [   1,  458,  269,  ...,    0,    0,    0]]),\n",
       " 'labels': tensor([[   1, 2467, 1280,  ...,    0,    0,    0],\n",
       "         [   1,  369,  274,  ...,    0,    0,    0],\n",
       "         [   1,  458,  490,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   1,  458,  269,  ...,    0,    0,    0],\n",
       "         [   1,  458,  338,  ...,    0,    0,    0],\n",
       "         [   1,  458,  269,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloaders[\"val\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlflow_experiment(\n",
    "    exp_name,\n",
    "    model_config_path,\n",
    "    dataloaders,\n",
    "    epochs\n",
    "):\n",
    "    with open(model_config_path, \"r\") as f:\n",
    "        model_config = json.load(f)\n",
    "    model_type = model_config.get(\"type\", \"No type in config\")\n",
    "    if model_type == \"lstm\":\n",
    "        model = models.lstm_models.build_SimpleBiLSTM(model_config)\n",
    "    elif model_type == \"residual_lstm\":\n",
    "        raise NotImplemented()\n",
    "    elif model_type == \"transformers\":\n",
    "        raise NotImplemented()\n",
    "    with mlflow.start_run(run_name=exp_name):\n",
    "        model.to(\"cuda\")\n",
    "        mlflow.log_params(model_config)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.set_tag(\"model_name\", model_type)\n",
    "        model = models.train_model.train_model(\n",
    "            model,\n",
    "            torch.nn.BCEWithLogitsLoss(),\n",
    "            optim.Adam(model.parameters()),\n",
    "            dataloaders,\n",
    "            epochs\n",
    "        )\n",
    "        models.eval_model.eval_model(model, dataloaders)\n",
    "        mlflow.pytorch.log_model(model, \"torch_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lstm1.json', 'lstm2.json', 'lstm0.json']\n"
     ]
    }
   ],
   "source": [
    "config_paths = \"../models/\"\n",
    "configs = [x for x in os.listdir(config_paths) if x.endswith(\".json\")]\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10\n",
      "----------\n",
      "Training batch 353/354\n",
      "Validation batch 0/76\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training batch 353/354\n",
      "Validation batch 0/76\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training batch 353/354\n",
      "Validation batch 0/76\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training batch 353/354\n",
      "Validation batch 0/76\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training batch 353/354\n",
      "Validation batch 0/76\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training batch 353/354\n",
      "Validation batch 0/76\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training batch 353/354\n",
      "Validation batch 0/76\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training batch 353/354\n",
      "Validation batch 0/76\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training batch 353/354\n",
      "Validation batch 0/76\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training batch 353/354\n",
      "Validation batch 0/76\n",
      "\n",
      "Training completed in 48m 55s\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'update'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conf \u001b[38;5;129;01min\u001b[39;00m configs:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mrun_mlflow_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_paths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m, in \u001b[0;36mrun_mlflow_experiment\u001b[0;34m(exp_name, model_config_path, dataloaders, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_type)\n\u001b[1;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mtrain_model\u001b[38;5;241m.\u001b[39mtrain_model(\n\u001b[1;32m     22\u001b[0m     model,\n\u001b[1;32m     23\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     epochs\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mpytorch\u001b[38;5;241m.\u001b[39mlog_model(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/etu/nn/paraphrase_detection/notebooks/../src/models/eval_model.py:27\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(model, dataloaders)\u001b[0m\n\u001b[1;32m     25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[0;32m---> 27\u001b[0m         \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m(outputs, labels)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric_name, metric \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     29\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, metric\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'update'"
     ]
    }
   ],
   "source": [
    "for conf in configs:\n",
    "    run_mlflow_experiment(\n",
    "        conf[:-5],\n",
    "        config_paths + conf,\n",
    "        dataloaders,\n",
    "        10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maslab_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
